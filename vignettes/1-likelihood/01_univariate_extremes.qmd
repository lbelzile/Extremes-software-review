---
title: "Likelihood inference for univariate extremes"
format: html
author: "Christophe Dutang, Leo Belzile"
---

```{r}
#| label: setup
#| echo: false
#| eval: true
library(ggplot2)
library(patchwork)
```

## Density and distribution function checks

We performed some sanity checks for various maximum likelihood estimation routine and parametric model implementations. Specifically, we verified that density functions are non-negative and evaluate to zero outside of the domain of the distribution, and that distribution functions are non-decreasing and map to the unit interval.

The generalized Pareto distribution has lower bound at the location parameter $u$ and is bounded above at $u -\sigma/\xi$ whenever $\xi < 0$. Many software implementations forgo the location parameter, since for modelling large quantiles of a random variable $Y$ above threshold $u$, it suffices to look at threshold exceedances $Y-u >0$. No threshold exceedance should be exactly equal to zero so the value of the density at that point is immaterial, even if it should be set to zero in practice.

```{r}
#| label: tbl-gp1
#| tbl-cap: "Evaluation of density and distribution function for the generalized Pareto model."
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: true
setwd(here::here())
options(knitr.kable.NA = '')
gpd_csv <- "outputs/2022_03_01_gpdpkglist-final.csv"
gpd_checks <- read.csv(file = gpd_csv)
gpd_checks |> 
  dplyrfilter(!is.na(fun)) |>
  dplyrselect(package,
                  location,
                  density.comment, 
                  distribution.comment) |>
  dplyrmutate(location = ifelse(is.na(location),
                                    "no",
                                    "yes")) |>
   dplyr::arrange("package") |>
  knitr::kable(col.names = c("package",
                                  "location",
                                  "density",
                                  "distribution function"),
                    row.names = FALSE)
```

```{r}
#| label: tbl-gev1
#| tbl-cap: "Evaluation of density and distribution function for the generalized extreme value distribuion."
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: true
#| error: false

options(knitr.kable.NA = '')
gev_csv <- "outputs/2022_03_01_gevpkglist-final.csv"
gev_checks <- read.csv(file = gev_csv)
gev_checks |> 
  dplyrfilter(!is.na(fun)) |>
  dplyrselect(package,
                  density.comment, 
                  distribution.comment) |>
   dplyr::arrange("package") |>
  knitr::kable(col.names = c("package",
                             "density",
                             "distribution function"),
                    row.names = FALSE)
```

Certain packages, listed in @tbl-gp1 and @tbl-gev1, have incorrect implementations of density and distribution functions.


## Optimization routines

We also compared the maximum likelihood estimates returned by default estimation procedures for different packages for simulated data, checking that the value returned is a global optimum and the gradient is approximately zero whenever $\widehat{\xi} > -1$. 


### Generalized Pareto distribution

For threshold exceedances, we simulated 
<!-- $n=200, \ldots, 900$ observations and set the threshold to the theoretical quantile of the distribution. We focus in the report on $n=400$, which leads to an average of 20 exceedances. 
-->
50 exceedances from a generalized Pareto distribution $\mathsf{GP}(\sigma=1000, \xi=-0.5)$ and from an exponential distribution with $\sigma=1000$. The large scale value is intended to check the robustness of gradient-based algorithms; from an optimization perspective, it is wise to ensure that the gradient of each component, scale and shape, is not too far apart in magnitude. The data could easily be scaled prior to the optimization in case this is problematic.


```{r}
#| label: fig-gpfit-comput
#| cache: true
#| echo: false
#| eval: true
#| message: false
#| warning: false
# library(dplyr, warn.conflicts = FALSE)
for(file in list.files(path = "outputs/", 
                       pattern = ".{11}GPD-fit.{3}.RData")){
   load(paste0("outputs/",file))
}

ids <- with(res_neg,
            which(apply(fullres[, "n20", "shape", ], 2, function(x) {
              sum(is.na(x))
            }) == dim(fullres)[1]))

# With 400 observations and a threshold set to the
# theoretical 95% percentile, we have on average
# 20 observations for the fit.
results <- res_neg$fullres[, "n50", , -ids]

g1 <- results[,  "gradshape", ] |>
  tibble::as_tibble() |>
  tidyr::pivot_longer(cols = tidyselect::everything(),
               names_to = "package",
               values_to = "gradient") |>
  dplyr::mutate(boundary = 
                  factor(rep(results[,  "shape", "mev"] < -0.9999, each = length(unique(package)))),
                ifelse(is.infinite(gradient), NA, gradient)) |>
  # filter(boundary == "no") |>
   dplyr::arrange("package") |>
  ggplot(mapping = aes(y = package,
                       x = log(abs(gradient)),
                       fill = boundary)) +
  scale_fill_manual(values = MetBrewer::met.brewer("OKeeffe1",
                                                   type = "discrete",
                                                   n = 2)) +
  ggdist::stat_slab(normalize = "xy", 
                    limits  = c(-30,30)) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(x = "log absolute gradient (shape)",
       y = "") +
  geom_vline(xintercept = log(0.001))

g2 <- results[,  "gradscale", ] |>
  tibble::as_tibble() |>
  tidyr::pivot_longer(cols = tidyselect::everything(),
               names_to = "package",
               values_to = "gradient") |>
  dplyr::mutate(boundary = factor(rep(results[, "shape", "mev"] < -0.9999, each = length(unique(package)))),
                package = factor(package)) |>
  # filter(boundary == "no") |>
  ggplot(mapping = aes(y = package,
                       x = log(abs(gradient)),
                       fill = boundary)) +
  scale_fill_manual(values = MetBrewer::met.brewer("OKeeffe1",
                                                   type = "discrete",
                                                   n = 2)) +
  ggdist::stat_slab(normalize = "xy", limits = c(-30,30)) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(x = "log absolute gradient (scale)",
       y = "") +
  geom_vline(xintercept = log(0.001))


# Find which is the maximum likelihood
shape_mle <- llmax <- scale_mle <- vector("numeric", 1000L)
wmax <- vector("integer", 1000L)
for (i in seq_along(shape_mle)) {
  wmax[i] <- which.max(results[i, "loglik", ])
  shape_mle[i] <- results[i, "shape", wmax[i]]
  scale_mle[i] <- results[i, "scale", wmax[i]]
  llmax[i] <- results[i, "loglik", wmax[i]]
}


g3a <- tibble::as_tibble(results[, "loglik", ] - llmax) |>
  tidyr::pivot_longer(cols = tidyselect::everything(),
               names_to = "package",
               values_to = "loglik") |>
  dplyr::filter(rep(results[, "shape", "mev"] >= -0.9999, each = length(unique(package)))) |>
  # filter(package != "tea") |> # outlier
  ggplot(mapping = aes(x = abs(loglik), y = package)) +
  geom_boxplot() +
  # ggdist::stat_slab() +
  theme_classic() +
  labs(x = "log likelihood difference to MLE",
       y = "")


g4a <- tibble::as_tibble(results[,  "shape", ]) |>
  tidyr::pivot_longer(cols = tidyselect::everything(),
               names_to = "package",
               values_to = "shape") |>
  # filter(package != "tea") |> # outlier
  ggplot(mapping = aes(x = shape, y = package)) +
  ggdist::stat_dots() +
  # geom_boxplot() +
  # scale_x_continuous(limits = c(-1, 2)) +
  # ggdist::stat_halfeye() +
  theme_classic() +
  labs(x = "shape",
       y = "")
results <- res_exp$fullres[, "n50", , -ids]
# Find which is the maximum likelihood
shape_mle <- llmax <- scale_mle <- vector("numeric", 1000L)
wmax <- vector("integer", 1000L)
for (i in seq_along(shape_mle)) {
  wmax[i] <- which.max(results[i, "loglik", ])
  shape_mle[i] <- results[i, "shape", wmax[i]]
  scale_mle[i] <- results[i, "scale", wmax[i]]
  llmax[i] <- results[i, "loglik", wmax[i]]
}

g3b <- tibble::as_tibble(results[, "loglik", ] - llmax) |>
  tidyr::pivot_longer(cols = tidyselect::everything(),
               names_to = "package",
               values_to = "loglik") |>
  dplyr::filter(rep(results[, "shape", "mev"] >= -0.9999, each = length(unique(package)))) |>
  # filter(package != "tea") |> # outlier
  ggplot(mapping = aes(x = abs(loglik), y = package)) +
  geom_boxplot() +
  # ggdist::stat_slab() +
  theme_classic() +
  labs(x = "log likelihood difference to MLE",
       y = "")


g4b <- tibble::as_tibble(results[,  "shape", ]) |>
  tidyr::pivot_longer(cols = tidyselect::everything(),
               names_to = "package",
               values_to = "shape") |>
  # filter(package != "tea") |> # outlier
  ggplot(mapping = aes(x = shape, y = package)) +
  ggdist::stat_dots() +
  # geom_boxplot() +
  # scale_x_continuous(limits = c(-1, 2)) +
  # ggdist::stat_halfeye() +
  theme_classic() +
  labs(x = "shape",
       y = "")
```

```{r}
#| label: fig-gpfit-grad
#| fig-cap: "Magnitude of the score vector at the value returned by the optimization routine. The density plots are based on 1000 samples simulated from a generalized Pareto distribution with shape $\\xi =-0.5$ and scale $\\sigma=1000$, split by simulations yielding a boundary case ($\\widehat{\\xi} = -1$, blue) and regular case ($\\widehat{\\xi} > -1$, red); the $y$-axis scale for each package is different to ease visualization. Results for samples for which the numerical routines failed to converge or the gradient is unevaluated are not shown. The vertical line indicates a tolerance of $0.001$"
#| cache: false
#| echo: false
#| eval: true
#| message: false
#| warning: false
g1 + g2
```

@fig-gpfit-grad shows the distribution of the score vector, i.e., the gradient of the log likelihood. The latter should vanish when evaluated at the maximum likelihood estimator ($\widehat{\sigma}, \widehat{\xi}$) provided $\widehat{\xi} > -1$. Most instances of non-zero gradient are attributable to boundary cases with $\widehat{\xi}=-1$ not accounted for. Other discrepancies are due to numerical tolerance for convergence, but the differences in log likelihood relative to the maximum over all routines are negligible in most non-boundary cases investigated. Some routines, based on Nelder--Mead simplex algorithm, do not check the gradient. This is immaterial if the value of the function is nearly identical to that of the maximum likelihood.

```{r}
#| label: fig-gpfit-diffmle
#| fig-cap: "Difference between likelihood evaluated at parameters returned by routine and the maximum likelihood over all routines for generalized Pareto samples with negative shape (left) and exponential samples (right), both with large scale parameter $\\sigma=1000$. Results for samples for which the numerical routines failed to converge are not shown."
#| cache: false
#| echo: false
#| eval: true
#| message: false
#| warning: false
g3a + g3b
```

@Fig-gpfit-diffmle shows these differences using box-and-whiskers plots, highlighting instances where the package fails to return correct values. Most packages do fine, except for a handful: `evd`, `extRemes` and `POT` stand out of the lot.

```{r}
#| label: fig-gpfit-shapepar
#| fig-cap: "Dot plots of shape parameters returned by optimization routine for generalized Pareto samples with negative shape (left) and exponential samples (right). Results for samples for which the numerical routines failed to converge are not shown."
#| cache: false
#| echo: false
#| eval: true
#| message: false
#| warning: false
g4a + g4b
```

We can figure out the source of some of these oddities by plotting the distribution of the shape parameter over all 1000 replications. @fig-gpfit-shapepar: the `QRM` package has unexpected small spread and a positive bias for estimation $\xi$, different from other packages because it fails more often when $\xi$ is negative. Both `ercv` and `extRemes` return routines that terminate at zero, with noticeable point masses at the origin. `Renext` returns a hard-coded lower bound, while only `SpatialExtremes` and `mev` correctly return $\xi=-1$. 

```{r}
#| label: tab-nonconv
#| tab-cap: "Number of failures (out of 1000 simulations) per package for bounded ($\\xi=-0.5$), light-tailed ($\\xi=0$) and heavy-tailed ($\\xi=0.5$) samples from a generalized Pareto distribution."
#| cache: false
#| echo: false
#| eval: true
#| message: false
#| warning: false
nonconv_neg <- res_neg$noncvg[,1,]
nonconv_neg <- nonconv_neg[,colSums(nonconv_neg)>0] |>
  tibble::as_tibble(rownames = "n") |>
  dplyr::mutate(n = factor(n, 
                    levels = c("n20","n50","n100","n1000"),
                     labels = c(20L,50L, 100L, 1000L))) |>
  tidyr::pivot_longer(cols = -1,
                      names_to = "package",
                      values_to = "nfail")

nonconv_exp <- res_exp$noncvg[,1,]
nonconv_exp <- nonconv_exp[,colSums(nonconv_exp)>0] |>
  tibble::as_tibble(rownames = "n") |>
  dplyr::mutate(n = factor(n, 
                    levels = c("n20","n50","n100","n1000"),
                     labels = c(20L,50L, 100L, 1000L))) |>
  tidyr::pivot_longer(cols = -1,
                      names_to = "package",
                      values_to = "nfail")

nonconv_pos <- res_pos$noncvg[,1,]
nonconv_pos <- nonconv_pos[,colSums(nonconv_pos)>0] |>
  tibble::as_tibble(rownames = "n") |>
  dplyr::mutate(n = factor(n, 
                    levels = c("n20","n50","n100","n1000"),
                     labels = c(20L,50L, 100L, 1000L))) |>
  tidyr::pivot_longer(cols = -1,
                      names_to = "package",
                      values_to = "nfail")
fail_df <- dplyr::bind_rows(.id = "shape", 
                            neg = nonconv_neg, 
                            zero = nonconv_exp, 
                            pos = nonconv_pos)
# knitr::kable(fail_df, row.names = "n", col.names = c("shape","package"))
fail_qrm <- as.integer(unlist((fail_df |> dplyr::filter(shape == "neg", package == "QRM") |> dplyr::select(nfail))))
fail_evir <- as.integer(fail_df |> dplyr::filter(shape == "neg", n == "20", package == "evir") |> dplyr::select(nfail))
```

Some package have routines that fail to converge often when the shape is negative; the most likely culprit for this is poor starting values. The routines in `ercv` and `fExtremes` (same as `evir`) fail often in small samples: for $n=20$ exceedances, the function returned an error in `r fail_evir[1]` simulations. For the latter, the error is due to poor implementation of the log-likelihood that leads to infinite finite differences between estimates. For `QRM`, the choice of starting values, which cannot be modified by the user, is not adequate with strong negative shapes: it failed more than `r fail_qrm[1]` ($n=20$), `r fail_qrm[2]` ($n=50$), `r fail_qrm[3]` ($n=100$) and `r fail_qrm[4]` ($n=1000$) for negative shapes, indicating that the issue is not sample size. The `qrmtools` package, which supersedes `QRM`, has no such problems.

### Generalized extreme value distribution

By contrast, the optimization routines for the generalized extreme value distribution are better behaved and nearly all packages give identical results: only `evd` and `texmex` failed to converge and returned abnormally high shape values in a handful of instances out of 1000 simulations.


```{r}
#| label: gevfit-computation
#| cache: true
#| echo: false
#| eval: true
#| message: false
#| warning: false
for(file in list.files(path = "outputs/", 
                       pattern = ".{11}GEV-fit-.{3}.RData")){
   load(paste0("outputs/",file))
}
results <- res_neg[, "n20",, ]
# 
# g1 <- results[,  "gradshape", ] |>
#   tibble::as_tibble() |>
#   tidyr::pivot_longer(cols = tidyselect::everything(),
#                names_to = "package",
#                values_to = "gradient") |>
#   dplyr::mutate(boundary = 
#                   factor(rep(results[,  "shape", "mev"] < -0.9999, each = length(unique(package)))),
#                 gradient = ifelse(is.infinite(gradient) | abs(gradient) > 1e10, NA, gradient)) |>
#   # filter(boundary == "no") |>
#   ggplot(mapping = aes(y = package,
#                        x = log(abs(gradient)),
#                        fill = boundary)) +
#   scale_fill_manual(values = MetBrewer::met.brewer("OKeeffe1",
#                                                    type = "discrete",
#                                                    n = 2)) +
#   ggdist::stat_slab(normalize = "xy", 
#                     limits  = c(-30,30)) +
#   theme_minimal() +
#   theme(legend.position = "none") +
#   labs(x = "log absolute gradient (shape)",
#        y = "") +
#   geom_vline(xintercept = log(0.001))+
#   scale_x_continuous(limits = c(-10,10))
# 
# g2 <- results[,  "gradscale", ] |>
#   tibble::as_tibble() |>
#   tidyr::pivot_longer(cols = tidyselect::everything(),
#                names_to = "package",
#                values_to = "gradient") |>
#   dplyr::mutate(boundary = factor(rep(results[, "shape", "mev"] < -0.9999, each = length(unique(package)))),
#                 package = factor(package)) |>
#   # filter(boundary == "no") |>
#   ggplot(mapping = aes(y = package,
#                        x = log(abs(gradient)),
#                        fill = boundary)) +
#   scale_fill_manual(values = MetBrewer::met.brewer("OKeeffe1",
#                                                    type = "discrete",
#                                                    n = 2)) +
#   ggdist::stat_slab(normalize = "xy", limits = c(-30,30)) +
#   theme_minimal() +
#   theme(legend.position = "none") +
#   labs(x = "log absolute gradient (scale)",
#        y = "") +
#   geom_vline(xintercept = log(0.001)) +
#   scale_y_continous(limits = c(-10,5))


# Find which is the maximum likelihood
shape_mle <- llmax <- scale_mle <- vector("numeric", 1000L)
wmax <- vector("integer", 1000L)
for (i in seq_along(shape_mle)) {
  wmax[i] <- which.max(results[i, "loglik", results[i, "shape", ] > -1])
  shape_mle[i] <- results[i, "shape", wmax[i]]
  scale_mle[i] <- results[i, "scale", wmax[i]]
  llmax[i] <- results[i, "loglik", wmax[i]]
}


g3a <- tibble::as_tibble(results[, "loglik", ] - llmax) |>
  tidyr::pivot_longer(cols = tidyselect::everything(),
               names_to = "package",
               values_to = "loglik") |>
  dplyr::filter(rep(results[, "shape", "mev"] >= -0.9999, each = length(unique(package)))) |>
  ggplot(mapping = aes(x = abs(loglik), y = package)) +
  geom_boxplot() +
  # ggdist::stat_slab() +
  theme_classic() +
  labs(x = "log likelihood difference to MLE",
       y = "") + 
  scale_x_continuous(limits = c(0, 15))
g4a <- tibble::as_tibble(results[,  "shape", ]) |>
  tidyr::pivot_longer(cols = tidyselect::everything(),
               names_to = "package",
               values_to = "shape") |>
  # filter(package != "tea") |> # outlier
  ggplot(mapping = aes(x = shape, y = package)) +
  ggdist::stat_dots() +
  # geom_boxplot() +
  # scale_x_continuous(limits = c(-1, 2)) +
  # ggdist::stat_halfeye() +
  theme_classic() +
  labs(x = "shape",
       y = "") +
  scale_x_continuous(limits = c(-2,1),oob = scales::squish)


results <- res_zer[, "n20", ,]
# Find which is the maximum likelihood
shape_mle <- llmax <- scale_mle <- vector("numeric", 1000L)
wmax <- vector("integer", 1000L)
for (i in seq_along(shape_mle)) {
  wmax[i] <- which.max(results[i, "loglik", ])
  shape_mle[i] <- results[i, "shape", wmax[i]]
  scale_mle[i] <- results[i, "scale", wmax[i]]
  llmax[i] <- results[i, "loglik", wmax[i]]
}

g3b <- tibble::as_tibble(results[, "loglik", ] - llmax) |>
  tidyr::pivot_longer(cols = tidyselect::everything(),
               names_to = "package",
               values_to = "loglik") |>
  dplyr::filter(rep(results[, "shape", "mev"] >= -0.9999, each = length(unique(package)))) |>
  # filter(package != "tea") |> # outlier
  ggplot(mapping = aes(x = abs(loglik), y = package)) +
  geom_boxplot() +
  # ggdist::stat_slab() +
  theme_classic() +
  labs(x = "log likelihood difference to MLE",
       y = "") +
  scale_x_continuous(limits = c(0,15))


g4b <- tibble::as_tibble(results[,  "shape", ]) |>
  tidyr::pivot_longer(cols = tidyselect::everything(),
               names_to = "package",
               values_to = "shape") |>
  # filter(package != "tea") |> # outlier
  ggplot(mapping = aes(x = shape, y = package)) +
  ggdist::stat_dots() +
  # geom_boxplot() +
  # scale_x_continuous(limits = c(-1, 2)) +
  # ggdist::stat_halfeye() +
  theme_classic() +
  labs(x = "shape",
       y = "") +
  scale_x_continuous(limits = c(-2,1),
                     oob = scales::squish)
```


```{r}
#| label: fig-gevfit-diffmle
#| fig-cap: "Difference between likelihood evaluated at parameters returned by routine and the maximum likelihood over all routines for generalized extreme value samples with negative shape (left) and exponential samples (right), both with large scale parameter $\\sigma=1000$. Results for samples for which the numerical routines failed to converge are not shown."
#| cache: false
#| echo: false
#| eval: true
#| message: false
#| warning: false
g3a + g3b
```

Unsurprisingly, the portrait is the same for the generalized extreme value distribution when it comes to boundary constraints: `climextRemes` does not return shapes less than $-1$, `extRemes` has odd behaviour with a visible point mass at $\xi=0$ in the simulations, even when this value has measure zero. Only `mev` and `SpatialExtremes` handle the boundary constraints. @fig-gevfit-diffmle shows the difference in maximum likelihood returned by the packages, excluding cases with $\widehat{\xi}=-1$ for which the log likelihood becomes unbounded for combinations of $\sigma$ and $\xi<-1$. Some packages, such as `evd`, also sometimes return local optimum and this in turn leads to erroneous comparisons of nested models.



```{r}
#| label: tbl-gevnonconv
#| tbl-cap: "Number of failures for the optimization routine for maximum likelihood-based estimation of the generalized extreme value model (out of 1000 simulations)."
#| tbl-subcap:
#|   - "bounded tail"
#|   - "light tail"
#|   - "heavy tail"
#| layout-ncol: 1
#| cache: true
#| eval: true
#| echo: false
#| message: false
#| error: false
for(file in list.files(path = "outputs/", 
                       pattern = ".{11}GEV-fit-.{3}.RData")){
   load(paste0("outputs/",file))
}
nonconv_neg <- apply(res_neg[,, "shape",], 2:3, function(x){sum(is.na(x))})
nonconv_neg <- nonconv_neg[,colSums(nonconv_neg)>0] |>
  tibble::as_tibble(rownames = "n") |>
  dplyr::mutate(n = factor(n, 
                    levels = c("n20","n50","n100","n1000"),
                     labels = c(20L,50L, 100L, 1000L))) #|>
  # tidyr::pivot_longer(cols = -1,
  #                     names_to = "package",
  #                     values_to = "nfail")
knitr::kable(nonconv_neg)
nonconv_zer <- apply(res_zer[,, "shape",], 2:3, function(x){sum(is.na(x))})
nonconv_zer <- nonconv_zer[,colSums(nonconv_zer)>0] |>
  tibble::as_tibble(rownames = "n") |>
  dplyr::mutate(n = factor(n, 
                    levels = c("n20","n50","n100","n1000"),
                     labels = c(20L,50L, 100L, 1000L))) #|>
  # tidyr::pivot_longer(cols = -1,
  #                     names_to = "package",
  #                     values_to = "nfail")
knitr::kable(nonconv_zer)
nonconv_pos <- apply(res_pos[,, "shape",], 2:3, function(x){sum(is.na(x))})
nonconv_pos <- nonconv_pos[,colSums(nonconv_pos)>0] |>
  tibble::as_tibble(rownames = "n") |>
  dplyr::mutate(n = factor(n, 
                    levels = c("n20","n50","n100","n1000"),
                     labels = c(20L,50L, 100L, 1000L))) #|>
#   tidyr::pivot_longer(cols = -1,
#                       names_to = "package",
#                       values_to = "nfail")
# fail_df <- dplyr::bind_rows(.id = "shape", 
#                             neg = nonconv_neg, 
#                             zero = nonconv_zer, 
#                             pos = nonconv_pos)
knitr::kable(nonconv_pos)
```
Relative to the generalized Pareto case, there are more instances of failure to converge, but these are restricted to a negligible number of cases except for negative shape with `QRM`: see @tbl-gevnonconv for a breakdown by package and sample size. 
